{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Web Scraping HTML using BeautifulSoup4.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPbDRhvTWmNFLAci1S9zusK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/crixspine/Web-Scraping-using-BeautifulSoup4/blob/master/Web_Scraping_HTML_using_BeautifulSoup4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6XOiPz2-mkj"
      },
      "source": [
        "import string\n",
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6aZJpnp-qBG"
      },
      "source": [
        "# Auto Workshops \n",
        "with address & contact no."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_LX6xbVrrdS"
      },
      "source": [
        "workshops = [] # raw html obj\n",
        "\n",
        "workshop_url_mod = [i for i in range (0, 1501, 100)] \n",
        "\n",
        "workshop_url_start = \"https://www.sgcarmart.com/directory/listing.php?BRSR=\"\n",
        "workshop_url_end = \"&CAT=7&ORD=name+asc&RPG=100\"\n",
        "\n",
        "for i in workshop_url_mod:\n",
        "  url = workshop_url_start + str(i) + workshop_url_end\n",
        "  page = requests.get(url)\n",
        "  soup = BeautifulSoup(page.content, 'html.parser')\n",
        "  results = soup.find_all(class_='font_bold roboto_sanserif')\n",
        "\n",
        "  for j in results:\n",
        "    workshops.append(j)\n",
        "\n",
        "workshops_clean = [str(i).split('>')[1].split('<')[0].replace('&amp;', '&') for i in workshops]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HFn0-9jSaCC"
      },
      "source": [
        "# Addresses\n",
        "\n",
        "workshops1 = [] # raw html obj\n",
        "\n",
        "workshops_url_mod = [0, 1501, 100]\n",
        "\n",
        "for i in workshop_url_mod:\n",
        "  url = workshop_url_start + str(i) + workshop_url_end\n",
        "  page = requests.get(url)\n",
        "  soup = BeautifulSoup(page.content, 'html.parser')\n",
        "  results = soup.find_all(width=510)\n",
        "\n",
        "  for j in results:\n",
        "    workshops1.append(j)\n",
        "\n",
        "workshops1_clean = [str(i).split('>')[1].split('<')[0].replace('&amp;', '&') for i in workshops1]\n",
        "workshops1_clean = [i for i in workshops1_clean if i.endswith('\\xa0(')]\n",
        "workshops1_clean = [i[:-2] for i in workshops1_clean]\n",
        "\n",
        "# some workshops do not have address listed\n",
        "# all available addresses are here"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZHHTF4smnWt"
      },
      "source": [
        "# Contact nos.\n",
        "\n",
        "import re\n",
        "\n",
        "workshops2_clean = [str(i).split('>')[1].split('<')[0].replace('&amp;', '&') for i in workshops1]\n",
        "workshops2_clean = [i for i in workshops2_clean if re.findall(\"^[0-9]{8}\", i)]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKdyzH9bAUcn"
      },
      "source": [
        "# Auto Surveyors / Appraisers\n",
        "with address"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZfZsVRVAT8g"
      },
      "source": [
        "items = []\n",
        "items2 = []\n",
        "url_mod = [1,2,3]\n",
        "\n",
        "surveyor_url_start = \"https://opencorpdata.com/sg?ssic=95303&page=\"\n",
        "\n",
        "for i in url_mod:\n",
        "  url = surveyor_url_start + str(i)\n",
        "  page = requests.get(url)\n",
        "  soup = BeautifulSoup(page.content, 'html.parser')\n",
        "  results = soup.find_all(\"a\")\n",
        "  for index, item in enumerate(results):\n",
        "    if index > 54:\n",
        "      items.append(item)\n",
        "\n",
        "items_clean = [str(i).split('>')[1].split('<')[0].replace('&amp;', '&') for i in items]\n",
        "items_clean = [i for i in items_clean if ((i != 'Next Page') and (i != 'Previous Page'))]\n",
        "\n",
        "\n",
        "for i in url_mod:\n",
        "  url = surveyor_url_start + str(i)\n",
        "  page = requests.get(url)\n",
        "  soup = BeautifulSoup(page.content, 'html.parser')\n",
        "  results = soup.find_all(\"td\")\n",
        "  for j in results:\n",
        "    items2.append(j)\n",
        "\n",
        "items2_clean = [str(i).split('>')[1].split('<')[0].replace('&amp;', '&') for i in items2]\n",
        "\n",
        "items2_clean = items2_clean[1::4]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2sh95hYHtUY"
      },
      "source": [
        "# Hospitals & Clinics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yE19MUywCDgO"
      },
      "source": [
        "# Clinics - SSIC 86201\n",
        "\n",
        "items = []\n",
        "\n",
        "url_mod = [i for i in range (1, 41)]\n",
        "\n",
        "surveyor_url_start = \"https://opencorpdata.com/sg?ssic=86201&page=\"\n",
        "\n",
        "for i in url_mod:\n",
        "  url = surveyor_url_start + str(i)\n",
        "  page = requests.get(url)\n",
        "  soup = BeautifulSoup(page.content, 'html.parser')\n",
        "  results = soup.find_all(\"a\")\n",
        "  for index, item in enumerate(results):\n",
        "    if index > 54:\n",
        "      items.append(item)\n",
        "\n",
        "items_clean = [str(i).split('>')[1].split('<')[0].replace('&amp;', '&') for i in items]\n",
        "items_clean = [i for i in items_clean if ((i != 'Next Page') and (i != 'Previous Page'))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YU2nZcVT-2Rp"
      },
      "source": [
        "# Law Firms LLC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssmHiE2p-9s1"
      },
      "source": [
        "# Legal Activities - SSIC 69100\n",
        "\n",
        "items = []\n",
        "\n",
        "url_mod = [i for i in range (1, 19)]\n",
        "\n",
        "surveyor_url_start = \"https://opencorpdata.com/sg?ssic=69100&page=\"\n",
        "\n",
        "for i in url_mod:\n",
        "  url = surveyor_url_start + str(i)\n",
        "  page = requests.get(url)\n",
        "  soup = BeautifulSoup(page.content, 'html.parser')\n",
        "  results = soup.find_all(\"a\")\n",
        "  for index, item in enumerate(results):\n",
        "    if index > 54:\n",
        "      items.append(item)\n",
        "\n",
        "items_clean = [str(i).split('>')[1].split('<')[0].replace('&amp;', '&') for i in items]\n",
        "items_clean = [i for i in items_clean if ((i != 'Next Page') and (i != 'Previous Page'))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rx5KmQI0_NLO"
      },
      "source": [
        "# Export"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHP2Fxehk6nn"
      },
      "source": [
        "workshops1_clean = [str(i).split('>')[1].split('<')[0].replace('&amp;', '&') for i in workshops1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LF6I9aFhmOco"
      },
      "source": [
        "workshops1_final = []\n",
        "for index, item in enumerate(workshops1_clean):\n",
        "  if str(item).endswith('\\xa0('):\n",
        "    workshops1_final.append(item)\n",
        "  else:\n",
        "    if (workshops1_clean[index-1] == '\\n') and (len(item) >= 10):\n",
        "      workshops1_final.append(item)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JO4vMW1blDlm",
        "outputId": "5a6b2b7e-d6f6-458d-9faa-f92b579d27c6"
      },
      "source": [
        "len(workshops1_final)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1521"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Or6hjcuR734I"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(workshops2_clean)\n",
        "df.to_excel(\"C:/Users/Admin/Desktop/test2.xlsx\")"
      ],
      "execution_count": 13,
      "outputs": []
    }
  ]
}